\begin{section}{Problem 1}

    \begin{problem}{1}
        Consider the $2 \times 2$ matrix
        $$
        A=\left( \begin{array}{cc}
        2 & -1   \\
        -1 & 2  \\
        \end{array} \right)
        \ ,
        $$
        and suppose we are required to solve $A \xx=\bb$, where $\bb$ is an arbitrary right-hand side vector. Clearly, solving a $2
        \times 2$ linear system using a stationary scheme is an utterly ridiculous idea, but the following is helpful in understanding more about convergence of stationary methods.
        \begin{enumerate}[(a)]
            \item Find the spectral radius of the Jacobi and Gauss-Seidel iteration matrices and the asymptotic rate of convergence for these two schemes, namely $-\log_{10} \rho(T)$, where $\rho(T)$ denotes the spectral radius of the corresponding iteration matrix, $T$. 
            \item How much faster does Gauss-Seidel converge compared to Jacobi for a fixed reduction in the relative residual norm, $\frac{\| \rr_k \|_2}{\| \bb \|_2}$, in terms of iteration counts?
            \item Write down the SOR iteration matrix as a function of the relaxation parameter, $\omega$.  
            \item Find the optimal SOR parameter, $\omega_{\rm opt}$, and the spectral radius of the corresponding iteration matrix.
            \item Approximately how much faster does SOR with $\omega_{\rm opt}$ converge compared to Jacobi?
        \end{enumerate}
    \end{problem}

    \begin{solution}{a}
        Given $A$ let us first calculate the Jacobi iteration matrix of $A$, denoted $D$. Since $D$ all zeros along the non-diagonal entries and shares the same diagonal entries as $A$ then we conclude that $D$ is the following. We also indicate its inverse below.
        \begin{align*}
            D = \begin{pmatrix}
                2 & 0 \\
                0 & 2 \\
            \end{pmatrix} & &
            D^{-1} = \begin{pmatrix}
                \frac{1}{2} & 0 \\
                0 & \frac{1}{2} \\
            \end{pmatrix}
        \end{align*}
        Since we have a computed value for $D^{-1}$ we can now calculate the iteration matrix $T_\text{Jacobi}$.
        \begin{align*}
            T_\text{Jacobi} &= I - D^{-1} A \\
            &= \begin{pmatrix}
                1 & 0 \\
                0 & 1 \\
            \end{pmatrix}
            - \begin{pmatrix}
                \frac{1}{2} & 0 \\
                0 & \frac{1}{2} \\
            \end{pmatrix}
            \begin{pmatrix}
                2 & -1 \\
                -1 & 2 \\        
            \end{pmatrix} \\
            &= \begin{pmatrix}
                0 & \frac{1}{2} \\
                \frac{1}{2} & 0 \\
            \end{pmatrix}
        \end{align*}
        \continued
        Now let us calculate the eigenvalues of $T_\text{Jacobi}$ to obtain the spectral radius.
        \begin{align*}
            \det \left( T_\text{Jacobi} - I \lambda \right) &= 0 \\
            \begin{vmatrix}
                -\lambda & \frac{1}{2} \\
                \frac{1}{2} & -\lambda \\
            \end{vmatrix} &= 0 \\
            \lambda^2 - \frac{1}{4} &= 0 \\
            \lambda &= \pm \frac{1}{2}
        \end{align*}
        We know that the largest absolute eigenvalue is $\frac{1}{2}$.
        \begin{align*}
            \rho \left( D^{-1} \right) = \lambda_1 = \frac{1}{2}
        \end{align*}
        Now that we have a value for $\rho \left( D^{-1} \right)$, we can calculate the asymptotic rate of convergence for the Jacobi iteration matrix of $A$ to be the following. \textit{Note: The following results were calculated using Matlab}.
        \begin{align*}
            - \log_{10} \rho \left( D^{-1} \right) = - \log_{10} \left( \frac{1}{2} \right) \approx \textbf{0.3010}
        \end{align*}
        We have now calculated the asymptotic rate of convergence for the Jacobi iteration matrix of $A$. Now let us calculate the asymptotic rate of convergence and the spectral radius for the Gauss-Seidel iteration matrix of $A$.
        \begin{align*}
            E &= \begin{pmatrix}
                2 & 0 \\
                -1 & 2
            \end{pmatrix} & 
            E^{-1} &= \frac{1}{2 \cdot 2 - 0 \cdot (-1)} \begin{pmatrix}
                2 & 0 \\
                1 & 2
            \end{pmatrix} = \begin{pmatrix}
                1/2 & 0 \\
                1/4 & 1/2
            \end{pmatrix}
        \end{align*}
        Since we now have a value for $E^{-1}$ we can now calculate the iteration matrix.
        \begin{align*}
            T_\text{GS} &= I - E^{-1} A \\
            &= \begin{pmatrix}
                1 & 0 \\
                0 & 1 \\
            \end{pmatrix}
            - \begin{pmatrix}
                1/2 & 0 \\
                1/4 & 1/2
            \end{pmatrix}
            \begin{pmatrix}
                2 & -1 \\
                -1 & 2 \\        
            \end{pmatrix} \\
            &= \begin{pmatrix}
                1 & 0 \\
                0 & 1 \\
            \end{pmatrix}
            - \begin{pmatrix}
                1 & -1/2 \\
                0 & 3/4 \\
            \end{pmatrix} \\
            &= \begin{pmatrix}
                0 & 1/2 \\
                0 & 1/4 \\
            \end{pmatrix} \\
        \end{align*}
        \continued
        Now that we have a value for $T_\text{GS}$ let us calculate its eigenvalues.
        \begin{align*}
            \det \left( T_\text{GS} - I \lambda \right) &= 0 \\
            \begin{vmatrix}
                -\lambda & \frac{1}{2} \\
                0 & 1/4 -\lambda \\
            \end{vmatrix} &= 0 \\
            -\lambda \cdot \left( \frac{1}{4} - \lambda \right) - \frac{1}{2} \cdot 0 &= 0 \\
            \lambda &= 0, \frac{1}{4}
        \end{align*}
        We know that the largest absolute eigenvalue is $\frac{1}{4}$.
        \begin{align*}
            \rho \left( E^{-1} \right) = \lambda_1 = \frac{1}{4}
        \end{align*}
        Now let us calculate the asymptotic rate of convergence.
        \begin{align*}
            - \log_{10} \rho \left( E^{-1} \right) = - \log_{10} \left( \frac{1}{4} \right) \approx \textbf{0.6021}
        \end{align*}
    \end{solution}

    \newpage

    \begin{solution}{b}
        Jacobi will take twice as many iterations as Gauss-Seidel since the asymptotic rate of Jacobi is roughly double that of Gauss-Seidel, thus we can say that Gauss-Seidel will converge at twice the rate that Jacobi will converge. 
    \end{solution}

    \newpage

    \begin{solution}{c}
        We know that the SOR iteration matrix $T_\text{SOR}$ is the following by definition.
        \begin{align*}
            I - \omega ((1 - \omega)D + \omega E)^{-1} A
        \end{align*}
        Thus we can compose it as the following function.
        \begin{align*}
            \boldmath{W}(\omega) &= I - \frac{1}{\omega} ((1 - \omega)D + \omega E) A
        \end{align*}
        \todo{write this up properly}
    \end{solution}

    \newpage

    \begin{solution}{d}
        \todo{use page 235 ch7 from textbook to solve this}
    \end{solution}

    \newpage 

    \begin{solution}{e}
        \todo{compare both of the values from d and a then compare}
    \end{solution}

\end{section}